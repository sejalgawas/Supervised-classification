{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Classification\n",
        "\n",
        "Q1. What is information gain and how it is used in decision trees\n",
        " - Information Gain is a metric used in decision trees to measure how much a feature helps to reduce uncertainty in the data, or \"entropy\".\n",
        " - It is calculated by subtracting the weighted average of the entropies of the child nodes from the entropy of the parent node. In decision trees, the feature with the highest Information Gain at each node is chosen to split the data, as it is the most effective for separating the different classes\n",
        "\n",
        "Q2. What is differnce bet Gini impurity and entropy\n",
        " - Gini Impurity\n",
        "   - Strengths:\n",
        "     - High speed: Its computational efficiency makes it a top choice for large-scale datasets and applications where training time is a priority.\n",
        "     - Simplicity: The formula is easier to understand and calculate, offering straightforward interpretability.\n",
        "   - Weaknesses:\n",
        "     - Bias: Can be biased towards favoring the majority class, potentially overlooking important splits for minority classes.\n",
        "     - Sensitivity to outliers: It can be more sensitive to noise in the data, which might influence the quality of splits.\n",
        " - Entropy\n",
        "   - Strengths:\n",
        "     - Robustness: Provides a more nuanced and theoretically-grounded measure of uncertainty, which can be more robust for certain complex datasets.\n",
        "     - Handles imbalanced data: Better at handling imbalanced datasets by producing more balanced and equitable splits.\n",
        "   - Weaknesses:\n",
        "     - Slower computation: The logarithmic function makes it slower to compute, which can be a significant drawback for large datasets.\n",
        "     - Interpretation: The concept of \"information gain\" based on entropy is more abstract than the misclassification probability of Gini, making it less intuitive for beginners.\n",
        "\n",
        " - Use Gini Impurity when:\n",
        "You are working with a large dataset and need to prioritize computational efficiency and speed.\n",
        "Your dataset has a fairly balanced class distribution.\n",
        "The interpretability of a probabilistic misclassification error is important.\n",
        " - Use Entropy when:\n",
        "You are working with a smaller dataset where the computational overhead is not a concern.\n",
        "Your dataset has a high degree of class imbalance and you want to ensure the tree handles minority classes effectively.\n",
        "You require the most theoretically pure split, even at a slight expense of speed.\n",
        "\n",
        "Q3. What is prepruning in decision tree\n",
        " - Pre-pruning, also known as early stopping, involves halting the growth of the decision tree before it becomes fully developed"
      ],
      "metadata": {
        "id": "NFKfzDMEHvcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Write a python program to train a decision tree classifier using Gini imputy as the criterion and printbthe feature importance '''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ9uUfBhKcGE",
        "outputId": "0cfd34c2-f275-4e15-bfef-853a26de0582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is svm\n",
        " - A support vector machine (SVM) is a supervised machine learning algorithm that classifies data by finding an optimal line or hyperplane that maximizes the distance between each class in an N-dimensional space\n",
        "\n",
        "Q6. What is the kernel Trick in svm\n",
        " - The kernel trick is a method used in Support Vector Machines (SVM) to classify non-linearly separable data by transforming it into a higher-dimensional space where it becomes linearly separable. Instead of explicitly calculating the coordinates in this new space, the kernel trick uses a kernel function to directly compute the dot products between the transformed data points, which is computationally cheaper and faster. This allows SVMs to find a linear \"hyperplane\" in the higher dimension that separates the data"
      ],
      "metadata": {
        "id": "t4yZka-MMw1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Write a python program to train two classifier with linear and RBF kernela on yhe wine dataset then compare their accuracies '''\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "\n",
        "print(f\"Accuracy with Linear Kernel: {acc_linear:.4f}\")\n",
        "print(f\"Accuracy with RBF Kernel: {acc_rbf:.4f}\")\n",
        "\n",
        "\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"\\nThe Linear kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"\\nThe RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\nBoth kernels performed equally well.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCDqoGt8NX7g",
        "outputId": "d5f069b0-89be-4419-f536-e00870a978c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.9815\n",
            "Accuracy with RBF Kernel: 0.7593\n",
            "\n",
            "The Linear kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is naive bayes classifier and why is it called naive\n",
        "\n",
        " - The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' theorem that is used for classification tasks. It is called \"naïve\" because it makes a strong and often unrealistic assumption that all features are independent of each other, meaning one feature's presence does not affect the presence of another\n",
        "\n",
        "Q9. Explain the diff between gaussian naive bayes multinominal naive bayes and bernoli naive bayes\n",
        "\n",
        " - Assumes features follow a Gaussian distribution and estimates the mean and variance for each class   Calculates the probability of each feature based on its frequency or count  Calculates the probability of a feature being present or absent\n",
        "Common Use Case Classification problems with continuous features like house price prediction or medical diagnosis   Text classification, such as spam detection, based on word frequencies  Text classification, like spam detection, based on the presence or absence of words, especially with shorter document"
      ],
      "metadata": {
        "id": "cXBThTQbPJlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''. Write a python program to train a gaussian naive bayes classifier on the breast cancer dataset and evaluate accuracy '''\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create and train Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Optional: show a few predicted vs actual values\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(\"Predicted:\", y_pred[:10])\n",
        "print(\"Actual:   \", y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL_bZI7BQItz",
        "outputId": "204b9e0d-45d8-4553-9512-fd094c5ef12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9415\n",
            "\n",
            "Sample Predictions:\n",
            "Predicted: [1 0 0 1 1 0 0 0 1 1]\n",
            "Actual:    [1 0 0 1 1 0 0 0 1 1]\n"
          ]
        }
      ]
    }
  ]
}